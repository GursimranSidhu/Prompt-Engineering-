{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/fal-ai/fal-ai/stable-diffusion-v35-large (Request ID: Root=1-687c7d2b-1e1b22ae28a528d12c6d731a;a192976a-65fc-495b-83e5-47cd73421434)\n\nCannot use free credits with provider fal-ai. Upgrade to PRO to use this provider.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/fal-ai/fal-ai/stable-diffusion-v35-large",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      5\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mHF_TOKEN\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mhf_EPdfKkSBPbUoFZHYWsZEsdqkLvpqyPALCu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m client = InferenceClient(\n\u001b[32m      8\u001b[39m     provider=\u001b[33m\"\u001b[39m\u001b[33mfal-ai\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     api_key=os.environ[\u001b[33m\"\u001b[39m\u001b[33mHF_TOKEN\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m image = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_to_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAstronaut riding a horse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstabilityai/stable-diffusion-3.5-large\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m image.save(\u001b[33m\"\u001b[39m\u001b[33moutput.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Image saved as output.png\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2491\u001b[39m, in \u001b[36mInferenceClient.text_to_image\u001b[39m\u001b[34m(self, prompt, negative_prompt, height, width, num_inference_steps, guidance_scale, model, scheduler, seed, extra_body)\u001b[39m\n\u001b[32m   2474\u001b[39m provider_helper = get_provider_helper(\u001b[38;5;28mself\u001b[39m.provider, task=\u001b[33m\"\u001b[39m\u001b[33mtext-to-image\u001b[39m\u001b[33m\"\u001b[39m, model=model_id)\n\u001b[32m   2475\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m   2476\u001b[39m     inputs=prompt,\n\u001b[32m   2477\u001b[39m     parameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m   2489\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m   2490\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2491\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2492\u001b[39m response = provider_helper.get_response(response)\n\u001b[32m   2493\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _bytes_to_image(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:280\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    277\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/fal-ai/fal-ai/stable-diffusion-v35-large (Request ID: Root=1-687c7d2b-1e1b22ae28a528d12c6d731a;a192976a-65fc-495b-83e5-47cd73421434)\n\nCannot use free credits with provider fal-ai. Upgrade to PRO to use this provider."
     ]
    }
   ],
   "source": [
    "# Replace this with your actual Hugging Face API token\n",
    "API_TOKEN = (\"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\")\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-3.5-large\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "    \"Accept\": \"image/png\",  # Expecting an image in return\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def generate_image(prompt: str, output_path: str = \"output.png\"):\n",
    "    payload = {\n",
    "        \"inputs\": prompt\n",
    "    }\n",
    "\n",
    "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"✅ Image saved as {output_path}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to generate image. Status code: {response.status_code}\")\n",
    "        print(response.text)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt_text = input(\"Enter your image prompt: \")\n",
    "    generate_image(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
